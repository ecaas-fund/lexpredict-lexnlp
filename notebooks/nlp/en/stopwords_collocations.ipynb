{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "import pickle\n",
    "import unicodedata\n",
    "\n",
    "# Sklearn imports\n",
    "import nltk\n",
    "import nltk.collocations\n",
    "\n",
    "from segmenters import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup default path for documents\n",
    "document_path = \"/data/workspace/lexpredict-contraxsuite-samples/\"\n",
    "document_type = \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_file_list(path, extension=None):\n",
    "    file_list = []\n",
    "    for file_name in os.listdir(path):\n",
    "        if os.path.isdir(os.path.join(path, file_name)):\n",
    "            file_list.extend(build_file_list(os.path.join(path, file_name)))\n",
    "        elif os.path.isfile(os.path.join(path, file_name)):\n",
    "            if extension and file_name.lower().endswith(extension.lower()):\n",
    "                file_list.append(os.path.join(path, file_name))\n",
    "            else:\n",
    "                file_list.append(os.path.join(path, file_name))\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File list\n",
    "file_list = build_file_list(document_path)\n",
    "file_list = file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document data\n",
    "document_data = []\n",
    "for file_name in file_list:\n",
    "    document_tokens = []\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as file_handle:\n",
    "        try:\n",
    "            file_buffer = file_handle.read()\n",
    "        except UnicodeDecodeError as e:\n",
    "            continue\n",
    "        \n",
    "        for sentence in get_sentences(file_buffer):\n",
    "            document_tokens.extend([t.lower() for t in nltk.word_tokenize(sentence) if t.isalnum()])\n",
    "    document_data.extend(document_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get raw token counts\n",
    "token_counts = pandas.Series(document_data).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', 'or', 'to', 'and', 'any', 'in', 'a', 'such', 'shall', 'by', 'be', 'as', 'for', 'with', 'borrower', 'agreement', 'other', 'this', 'all', 'that', 'is', 'on', 'section', 'not', 'lender', 'its', 'which', 'under', 'date', 'agent', 'each', 'at', 'company', 'may', 'loan', 'time', 'bank', 'from', 'an', 'have', 'if', 'interest', 'amount', 's', 'credit', 'contractor', 'will', 'b', 'are', 'no', 'rate', 'owner', 'payment', 'provided', 'has', 'respect', 'period', 'obligations', 'applicable', 'party', 'notice', 'documents', 'i', 'including', 'it', 'than', 'business', 'upon', 'event', 'means', 'made', 'required', 'person', 'pursuant', 'ii', 'executive', 'after', 'administrative', 'c', 'without', 'thereof', 'day', 'page', 'otherwise', 'hereunder', 'loans', 'default', 'been', 'terms', 'accordance', 'property', 'work', 'days', 'effect', 'construction', 'set', 'law', 'material', 'forth']\n"
     ]
    }
   ],
   "source": [
    "print(token_counts.head(100).index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stopwords: 163\n"
     ]
    }
   ],
   "source": [
    "# Setup stopwords from corpus\n",
    "stopwords = set(list(nltk.corpus.stopwords.words(\"english\")))\n",
    "stopwords.update(['the', 'of', 'or', 'to', 'and', 'any', 'in', 'a', 'such', 'shall', 'by', 'be', 'as', 'for',\n",
    "                  'with', 'other', 'this', 'all', 'that', 'is', 'on', 'not', 'its', 'which', 'under', 'each',\n",
    "                  'at', 'may', 'from', 'an', 'have', 'if', 's', 'will', 'b', 'are', 'no', 'has', \n",
    "                  'i', 'including', 'it', 'than', 'upon', 'after', 'without', 'thereof', 'otherwise', 'hereunder', \n",
    "                  'been', 'forth'])\n",
    "print(\"Total stopwords: {0}\".format(len(stopwords)))\n",
    "\n",
    "# Save the tokenizer\n",
    "with open(\"stopwords.pickle\", \"wb\") as out_file:\n",
    "    pickle.dump(stopwords, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup bigram finder\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(document_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_list = [100, 1000, 10000]\n",
    "\n",
    "for n in top_n_list:\n",
    "    # Apply filter and output\n",
    "    finder.apply_freq_filter(0.001 * len(document_data))\n",
    "    bigram_collocations = list(finder.nbest(bigram_measures.pmi, n))\n",
    "\n",
    "    # Save the tokenizer\n",
    "    with open(\"collocation_bigrams_{0}.pickle\".format(n), \"wb\") as out_file:\n",
    "        pickle.dump(bigram_collocations, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup bigram finder\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_words(document_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_n_list = [100, 1000, 10000]\n",
    "\n",
    "for n in top_n_list:\n",
    "    # Apply filter and output\n",
    "    finder.apply_freq_filter(0.0001 * len(document_data))\n",
    "    trigram_collocations = list(finder.nbest(trigram_measures.pmi, n))\n",
    "\n",
    "    # Save the tokenizer\n",
    "    with open(\"collocation_trigrams_{0}.pickle\".format(n), \"wb\") as out_file:\n",
    "        pickle.dump(trigram_collocations, out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
