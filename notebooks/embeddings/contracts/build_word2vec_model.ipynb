{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tika\n",
    "import tika\n",
    "tika.initVM()\n",
    "from tika import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/alex/dev/michael/contraxsuite/lexpredict-contraxsuite-core/notebooks/embeddings/contracts'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/dev/michael/contraxsuite/cve/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# NLP/ML imports\n",
    "sys.path.append(\"../../../lexnlp/\")\n",
    "import nlp.en.tokens\n",
    "from nlp.en.segments.sentences import get_sentences\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup default path for documents\n",
    "document_path = \"/home/alex/Desktop/test_folder/\"\n",
    "document_type = \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_file_list(path, extension=None):\n",
    "    file_list = []\n",
    "    for file_name in os.listdir(path):\n",
    "        if os.path.isdir(os.path.join(path, file_name)):\n",
    "            file_list.extend(build_file_list(os.path.join(path, file_name)))\n",
    "        elif os.path.isfile(os.path.join(path, file_name)):\n",
    "            if extension and file_name.lower().endswith(extension.lower()):\n",
    "                file_list.append(os.path.join(path, file_name))\n",
    "            else:\n",
    "                file_list.append(os.path.join(path, file_name))\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on 5 files...\n",
      "found 13 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Iterate through file list            \n",
    "file_list = build_file_list(document_path)\n",
    "file_list = file_list\n",
    "\n",
    "sentence_list = []\n",
    "\n",
    "print(\"training on {0} files...\".format(len(file_list)))\n",
    "for file_name in file_list:\n",
    "    # Load document\n",
    "    tika_response = tika.parser.from_file(os.path.join(document_path, file_name))\n",
    "    try:\n",
    "        tika_content = unidecode.unidecode(tika_response[\"content\"])\n",
    "    except KeyError as e:\n",
    "        continue\n",
    "    \n",
    "    # Parse into sentences and tokens\n",
    "    for sentence in get_sentences(tika_content):\n",
    "        sentence_list.append(nlp.en.tokens.get_lemmas(sentence, lowercase=True))        \n",
    "\n",
    "print(\"found {0} sentences.\".format(len(sentence_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-19-28d631ecece4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Train w2v CBOW model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mw2v_model_cbow\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgensim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mword2vec\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mWord2Vec\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence_list\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m200\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwindow\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m20\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmin_count\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mworkers\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mw2v_model_cbow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"w2v_model_cbow\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cbow trained.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/dev/michael/contraxsuite/cve/lib/python3.6/site-packages/gensim/models/word2vec.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001B[0m\n\u001B[1;32m    598\u001B[0m             \u001B[0msentences\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msentences\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcorpus_file\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcorpus_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mworkers\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mworkers\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvector_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0miter\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    599\u001B[0m             \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_words\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbatch_words\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrim_rule\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrim_rule\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msg\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0malpha\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0malpha\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwindow\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mwindow\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 600\u001B[0;31m             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss)\n\u001B[0m\u001B[1;32m    601\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    602\u001B[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001B[0;32m~/dev/michael/contraxsuite/cve/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, **kwargs)\u001B[0m\n\u001B[1;32m    747\u001B[0m                 \u001B[0msentences\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msentences\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcorpus_file\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcorpus_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtotal_examples\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcorpus_count\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    748\u001B[0m                 \u001B[0mtotal_words\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcorpus_total_words\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_alpha\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malpha\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 749\u001B[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001B[0m\u001B[1;32m    750\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    751\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtrim_rule\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/dev/michael/contraxsuite/cve/lib/python3.6/site-packages/gensim/models/word2vec.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001B[0m\n\u001B[1;32m    725\u001B[0m             \u001B[0msentences\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msentences\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcorpus_file\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcorpus_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtotal_examples\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtotal_examples\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtotal_words\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtotal_words\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    726\u001B[0m             \u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_alpha\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstart_alpha\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_alpha\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mend_alpha\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mword_count\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mword_count\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 727\u001B[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001B[0m\u001B[1;32m    728\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    729\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mscore\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msentences\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtotal_sentences\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1e6\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mchunksize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mqueue_factor\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreport_delay\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/dev/michael/contraxsuite/cve/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m   1065\u001B[0m             \u001B[0mtotal_words\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtotal_words\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_alpha\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstart_alpha\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_alpha\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mend_alpha\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mword_count\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mword_count\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1066\u001B[0m             \u001B[0mqueue_factor\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mqueue_factor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreport_delay\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreport_delay\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcompute_loss\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcompute_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1067\u001B[0;31m             **kwargs)\n\u001B[0m\u001B[1;32m   1068\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1069\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_get_job_params\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcur_epoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/dev/michael/contraxsuite/cve/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    533\u001B[0m             \u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    534\u001B[0m             \u001B[0mtotal_examples\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtotal_examples\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 535\u001B[0;31m             total_words=total_words, **kwargs)\n\u001B[0m\u001B[1;32m    536\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    537\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mcallback\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/dev/michael/contraxsuite/cve/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001B[0m in \u001B[0;36m_check_training_sanity\u001B[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001B[0m\n\u001B[1;32m   1171\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1172\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvocab\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# should be set by `build_vocab`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1173\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"you must first build vocabulary before training the model\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1174\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvectors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1175\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"you must initialize vectors before training the model\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "# Train w2v CBOW model\n",
    "w2v_model_cbow = Word2Vec(sentence_list, vector_size=200, window=20, min_count=10, workers=2)\n",
    "w2v_model_cbow.save(\"w2v_model_cbow\")\n",
    "print(\"cbow trained.\")\n",
    "\n",
    "# Train w2v CBOW model\n",
    "w2v_model_sg = Word2Vec(sentence_list, vector_size=200, window=20, min_count=10, workers=2, sg=1)\n",
    "w2v_model_sg.save(\"w2v_model_sg\")\n",
    "print(\"sg trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = [list(i) for i in sentence_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}