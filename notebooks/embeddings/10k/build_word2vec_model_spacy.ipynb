{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import glob\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import zlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup tika\n",
    "import tika\n",
    "tika.initVM()\n",
    "from tika import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLP/ML imports\n",
    "import spacy\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# Iterate through tar files\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'be', 'an', 'example', 'sentence'],\n",
       " ['and', 'yet', 'another', 'here']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_sentences(text):\n",
    "    sentence_list = []\n",
    "    \n",
    "    # Create spacy document\n",
    "    doc = nlp(text)\n",
    "    for sentence in doc.sents:\n",
    "        sentence_list.append([t.lemma_ for t in sentence if t.lemma_.isalnum()])\n",
    "\n",
    "    return sentence_list\n",
    "\n",
    "extract_sentences(\"This is an example sentence.  And yet another here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/filings_10k_1994.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Setup file\n",
    "for year in range(1994, 1997):\n",
    "    # Get file name and open\n",
    "    file_name = \"data/filings_10k_{0}.tar.gz\".format(year)\n",
    "    print(file_name)\n",
    "    tar_file = tarfile.open(file_name)\n",
    "\n",
    "    # Sample data\n",
    "    sample_sentence_list = []\n",
    "\n",
    "    print(\"building sample...\")\n",
    "    \n",
    "    # Iterate through members\n",
    "    j = 0\n",
    "    for tar_member in tar_file.getmembers():\n",
    "        if j % 1000 == 0:\n",
    "            print((year, j, len(sample_sentence_list)))\n",
    "\n",
    "        j += 1\n",
    "        # Skip non-files\n",
    "        if not tar_member.isfile():\n",
    "            continue\n",
    "\n",
    "        # Parse real files\n",
    "        try:\n",
    "            # Read tar data\n",
    "            member_buffer = zlib.decompress(tar_file.extractfile(tar_member).read())\n",
    "\n",
    "            # Send to tika\n",
    "            filing_buffer = parser.from_buffer(member_buffer)\n",
    "            if 'content' in filing_buffer:\n",
    "                filing_buffer = filing_buffer['content']\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Parse\n",
    "            #print((tar_member, len(tar_data), len(filing_buffer), len(sample_sentence_list)))\n",
    "\n",
    "            # Get sentence list\n",
    "            sample_sentence_list.extend(extract_sentences(filing_buffer))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    print(\"training w2v models...\")\n",
    "    # Train w2v CBOW model\n",
    "    w2v_model_cbow = Word2Vec(sample_sentence_list, vector_size=200, window=20, min_count=10, workers=2)\n",
    "    w2v_model_cbow.save(\"w2v_model_cbow_{0}\".format(year))\n",
    "    print(\"cbow trained.\")\n",
    "    \n",
    "    # Train w2v SG model\n",
    "    w2v_model_sg = Word2Vec(sample_sentence_list, vector_size=200, window=20, min_count=10, workers=2, sg=1)\n",
    "    w2v_model_sg.save(\"w2v_model_sg_{0}\".format(year))\n",
    "    print(\"sg trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}